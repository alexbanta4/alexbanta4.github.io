{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7cfoFakEcep"
      },
      "source": [
        "\n",
        "# This is the main torch package\n",
        "import torch \n",
        "#Computer vision specific package              \n",
        "import torchvision\n",
        "#There are a bunch of standard datasets in torchvision. \n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "\n",
        "# Librosa for processing datasets\n",
        "\n",
        "import librosa as lb\n",
        "\n",
        "# Import Google Drive Access\n",
        "from google.colab import files\n",
        "\n",
        "# OS and Pickle Imports for saving CQT representations and trained network in drive\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSmJJ4mQEjCP"
      },
      "source": [
        "## Data Helper Functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOOP1H_S3igc"
      },
      "source": [
        "# Google Drive mounting for data access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbOSmwgyS1sL"
      },
      "source": [
        "# Convert .mp3 files from datasets into Constant-Q transforms\n",
        "def song_to_cqt(song_path):\n",
        "  song, sampling_rate = lb.load(song_path)\n",
        "  C= lb.cqt(song,sampling_rate)\n",
        "  desire_spect_len = 2580\n",
        "  # if spectral respresentation too long, crop it, otherwise, zero-pad\n",
        "  if C.shape[1] >= desire_spect_len:\n",
        "    C = C[:,0:desire_spect_len]\n",
        "  else:\n",
        "    C = np.pad(C,((0,0),(0,desire_spect_len-C.shape[1])), 'constant')\n",
        "  return C\n",
        "\n",
        "# Create a Matrix for every song to keep track of which song has which CQT representation\n",
        "# {Key: 'Song', Value: CQT representation}\n",
        "def create_feature_matrix(song_folder_path):\n",
        "  reference_feature_matrix = {}\n",
        "  cover_feature_matrix = {}\n",
        "  i = 0\n",
        "  for songname in os.listdir(song_folder_path):\n",
        "    if songname!='.DS_Store':\n",
        "      song_folder = song_folder_path + '/' + songname\n",
        "      for song in os.listdir(song_folder):\n",
        "        song_path = song_folder + '/' + song\n",
        "        if song.find('Cover') != -1:\n",
        "          cover_feature_matrix[song] = song_to_cqt(song_path)\n",
        "        else: \n",
        "          ref_song = song[0:song.find('Original')]\n",
        "          reference_feature_matrix[ref_song] = song_to_cqt(song_path)\n",
        "    i = i + 1\n",
        "    print(i)\n",
        "  return cover_feature_matrix, reference_feature_matrix\n",
        "\n",
        "#Save Feature Matrix to drive so songs only need to be converted to CQT once\n",
        "def save_feature_matrix(song_folder_path, save_path):\n",
        "  cover_songs, reference_songs = create_feature_matrix(song_folder_path)\n",
        "  print(\"\")\n",
        "  print('Saving Feature Matrix')\n",
        "  r_outfile = open(save_path + 'references','wb')\n",
        "  pickle.dump(reference_songs, r_outfile)\n",
        "  r_outfile.close()\n",
        "  c_outfile = open(save_path + 'covers','wb')\n",
        "  pickle.dump(cover_songs, c_outfile)\n",
        "  c_outfile.close()\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV09odhjOkFN"
      },
      "source": [
        "# Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20uUBpbYpf_0"
      },
      "source": [
        "### Run this Cell only once ###\n",
        "# Saves CQT of covers and reference songs to Google Drive \n",
        "train_set_path = '/content/drive/MyDrive/Deep Learning Final Project/secondhandsongs'\n",
        "train_store_path = '/content/drive/MyDrive/Deep Learning Final Project/train_data/'\n",
        "\n",
        "save_feature_matrix(train_set_path, train_store_path)\n",
        "\n",
        "\n",
        "print('Load Completed')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Kj4HWuEHPgM"
      },
      "source": [
        "### Run Every Time Notebook Resets ###\n",
        "\n",
        "# Load Cover Song and Refernce Song CQT representation from Google Drive\n",
        "\n",
        "covers_path = '/content/drive/MyDrive/Deep Learning Final Project/train_data/covers'\n",
        "references_path = '/content/drive/MyDrive/Deep Learning Final Project/train_data/references'\n",
        "\n",
        "infile = open(covers_path,'rb')\n",
        "cover_songs = pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "infile = open(references_path,'rb')\n",
        "reference_songs = pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "\n",
        "# Convert Feature Matrix to training data (tuples of song pairs) and labels (0 if cover, 1 if not cover)\n",
        "\n",
        "train_data = []\n",
        "train_labels = np.array([])\n",
        "for reference_song in reference_songs:\n",
        "  for cover_song in cover_songs:\n",
        "      label = int(cover_song.find(reference_song) == -1)\n",
        "      train_labels = np.append(train_labels,label)\n",
        "      song_tuple = (torch.from_numpy(cover_songs[cover_song]), torch.from_numpy(reference_songs[reference_song]))\n",
        "      train_data.append(song_tuple)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWYUuh7YxIfC"
      },
      "source": [
        "# Custom PyTorch Dataset for Songs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0ksFDZly66k"
      },
      "source": [
        "from torch.utils.data import DataLoader,Dataset\n",
        "import random\n",
        "\n",
        "# Custom dataset for cover song identification\n",
        "# (song0,song1) are from the reference set and cover set respectively\n",
        "# Label is 0 if song1 is a cover of song 0\n",
        "class CoverSongDataset(Dataset):\n",
        "    \n",
        "    def __init__(self,data,labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        #we need to make sure approx 50% of song pairs are cover/reference pairs\n",
        "        should_be_cover = random.randint(0,1) \n",
        "\n",
        "        if should_be_cover:\n",
        "          while True:\n",
        "            rand_ind = random.randint(0,len(self.labels)-1)\n",
        "            if self.labels[rand_ind] == 0:\n",
        "              song0,song1 = self.data[rand_ind]\n",
        "              lab = self.labels[rand_ind]\n",
        "              break\n",
        "        else: \n",
        "          while True:\n",
        "            rand_ind = random.randint(0,len(self.labels)-1)\n",
        "            if self.labels[rand_ind] == 1:\n",
        "              song0,song1 = self.data[rand_ind]\n",
        "              lab = self.labels[rand_ind]\n",
        "              break\n",
        "\n",
        "        return song0, song1 , torch.from_numpy(np.array([lab],dtype=np.float32))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jaSm1jlEpUM"
      },
      "source": [
        "# Siamese Network Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xVbEa2Nd3rW"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import PIL.ImageOps    \n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Siamese Convolutional Network \n",
        "# Input is two songs, output is a predicted label of that song\n",
        "class Siamese(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Siamese, self).__init__()\n",
        "      self.main = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,4)),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Dropout(.3),\n",
        "            nn.Conv2d(128, 96, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((3,5)),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.Dropout(.5),\n",
        "            nn.Conv2d(96, 64, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((3,8)),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(.5),\n",
        "            nn.Conv2d(64, 32, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(.5),\n",
        "        )\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "      self.fc1 = nn.Linear(32*13,416)\n",
        "      self.fcOut = nn.Linear(416,1)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.main(x1)\n",
        "        x1= x1.view(-1,32*13)\n",
        "        x1 = self.sigmoid(self.fc1(x1))\n",
        "\n",
        "        x2 = self.main(x2)\n",
        "        x2= x2.view(-1,32*13)\n",
        "        x2 = self.sigmoid(self.fc1(x2))\n",
        "        x = torch.abs(x1 - x2)\n",
        "        x= self.fcOut(x)\n",
        "        return x\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-OP8ryvfSAQ"
      },
      "source": [
        "# Training the Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxhCRGaBfg_L"
      },
      "source": [
        "# Define training parameters\n",
        "\n",
        "siamese_dataset = CoverSongDataset(train_data, train_labels)\n",
        "\n",
        "net = Siamese().cuda()\n",
        "criterion = F.mse_loss\n",
        "optimizer = optim.Adam(net.parameters(),lr = 0.001 )\n",
        "\n",
        "num_epochs=5\n",
        "\n",
        "train_dataloader = DataLoader(siamese_dataset,\n",
        "                        batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdJaUPp3gWd2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epoch_avg_loss = []\n",
        "epoch_med_loss = []\n",
        "x = []\n",
        "loss_mat = []\n",
        "\n",
        "# Train over 5 epoch with a batch size of 16\n",
        "\n",
        "for epoch in range(0,num_epochs):\n",
        "  print(epoch)\n",
        "  net.train()\n",
        "  for i, data in enumerate(train_dataloader, 0):\n",
        "        song0,song1,label = data\n",
        "        song0, song1 , label = song0.cuda(), song1.cuda() , label.cuda()\n",
        "        song0 = song0.unsqueeze(1)\n",
        "        song1 = song1.unsqueeze(1)\n",
        "        output = net(torch.abs(song0).float(),torch.abs(song1).float())\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output,label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_mat.append(loss.item())\n",
        "\n",
        "        # Calculate loss every 500 steps for plot\n",
        "        \n",
        "        if i%500==0:\n",
        "          avg_loss = np.mean(loss_mat)\n",
        "          med_loss = np.median(loss_mat)\n",
        "          epoch_avg_loss.append(avg_loss)\n",
        "          epoch_med_loss.append(med_loss)\n",
        "          curr_step = 3816 * epoch + i\n",
        "          x.append(curr_step)\n",
        "          print(str(curr_step))\n",
        "          print(avg_loss)\n",
        "          print(med_loss)\n",
        "          loss_mat = []\n",
        "\n",
        "  print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(x,epoch_avg_loss,label='Mean Loss')\n",
        "plt.plot(x,epoch_med_loss, label='Median Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv6awTq18b3l"
      },
      "source": [
        "# Save Trained Model to Google Drive\n",
        "\n",
        "save_path = references_path = '/content/drive/MyDrive/Deep Learning Final Project/model/final_model'\n",
        "\n",
        "torch.save(net,save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gMP-KYuJYnx"
      },
      "source": [
        "# Load Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iqlmuMXRsmE"
      },
      "source": [
        "## Only run once ##\n",
        "\n",
        "# Load in Covers80 dataset and save feature matrix to Drive #\n",
        "\n",
        "test_set_path = '/content/drive/MyDrive/Deep Learning Final Project/covers80/'\n",
        "test_store_path = '/content/drive/MyDrive/Deep Learning Final Project/test_data/test_refs'\n",
        "\n",
        "\n",
        "save_feature_matrix(test_set_path, test_store_refs_path)\n",
        "\n",
        "\n",
        "\n",
        "print('Load Completed')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2w0LJRQRs-k"
      },
      "source": [
        "# Run every time notebook refresehes #\n",
        "\n",
        "# Load CQT representations of Covers80 dataset\n",
        " \n",
        "covers_path = '/content/drive/MyDrive/Deep Learning Final Project/test_data/test_refscovers'\n",
        "references_path = '/content/drive/MyDrive/Deep Learning Final Project/test_data/test_refsreferences'\n",
        "\n",
        "infile = open(covers_path,'rb')\n",
        "test_cover_songs = pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "infile = open(references_path,'rb')\n",
        "test_reference_songs = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEE91kEhTqc5"
      },
      "source": [
        "# Testing the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT19fTy2qcvb"
      },
      "source": [
        "save_path = references_path = '/content/drive/MyDrive/Deep Learning Final Project/model/final_model'\n",
        "\n",
        "# Load Trained Network\n",
        "model = torch.load(save_path)\n",
        "model.eval()\n",
        "\n",
        "count = 0\n",
        "\n",
        "rank_mat = []\n",
        "\n",
        "# For every reference song, compare to every cover song\n",
        "\n",
        "for reference_song in test_reference_songs:\n",
        "  song0 = torch.from_numpy(test_reference_songs[reference_song]).cuda()\n",
        "  song0 = song0.unsqueeze(0)\n",
        "  song0 = song0.unsqueeze(0)\n",
        "  data = []\n",
        "  for cover_song in test_cover_songs:\n",
        "    song1 = torch.from_numpy(test_cover_songs[cover_song]).cuda()\n",
        "    song1 = song1.unsqueeze(0)\n",
        "    song1 = song1.unsqueeze(0)\n",
        "    if cover_song.find(reference_song) == -1:\n",
        "      test_label = 1\n",
        "    else: \n",
        "      test_label = 0\n",
        "    with torch.no_grad():\n",
        "      # Input (reference,cover) to the network \n",
        "      output = model(torch.abs(song0).float(),torch.abs(song1).float())\n",
        "      output_label_tuple = (test_label,output)\n",
        "      data.append(output_label_tuple)\n",
        "  # Rank reprsentation from lowest to highest of every cover song versus the reference song    \n",
        "  data.sort(key=lambda pair: pair[1])\n",
        "\n",
        "  # Find index of true cover song\n",
        "  rank = [x for x, y in enumerate(data) if y[0] == 0]\n",
        "  if len(rank)>1:\n",
        "    rank.sort()\n",
        "  rank_mat.append(rank[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3n7GFswBiA_"
      },
      "source": [
        "# Create boxplot of rankings of true cover song for all reference songs\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.boxplot(rank_mat)\n",
        "plt.title(\"Rank of Cover Songs\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}